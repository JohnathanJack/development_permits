{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.shapeyourcity.ca/development'\n",
    "PATH = \"C:\\Program Files (x86)\\chromedriver-win64\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_urls(webpage,path):\n",
    "    \"\"\"\n",
    "    Scrapes all the URLs of the development permits within the webpage\n",
    "\n",
    "    Parameters:\n",
    "        webpage (str): the url of the webpage\n",
    "        path (str): the location of the user's chromedriver\n",
    "\n",
    "    Returns:\n",
    "        A list of strings which contains all of the development permits url \n",
    "    \"\"\"\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service = Service(path), options=chrome_options)\n",
    "    driver.get(webpage)\n",
    "    # The element is located within an iframe, required to locate the iframe and switch frames\n",
    "    iframe = driver.find_element(By.TAG_NAME, 'iframe')\n",
    "    url_list = []\n",
    "    driver.switch_to.frame(iframe)\n",
    "    page_num = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \".chakra-button.ehq-paginationButton.css-i1louw\")))\n",
    "    last_page_num = int([item.text for item in page_num][-2])\n",
    "    # Scrape all of the urls on each page\n",
    "    for num in range(last_page_num+1):\n",
    "        # Ensures that all the CSS elements are loaded before scraping\n",
    "        urls = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, '.chakra-link.ehq-projectCoverImg.css-1eh7kaa')))\n",
    "        for url in urls:\n",
    "            url_list.append(url.get_attribute('href'))\n",
    "        # After scraping all of the elements, click to the next page if not on the final page\n",
    "        if num < last_page_num+1:\n",
    "            click = driver.find_element(By.CSS_SELECTOR, \".chakra-button.ehq-paginationButton.ehq-paginationNextButton.css-i1louw\")\n",
    "            click.click()\n",
    "        else:\n",
    "            break\n",
    "        time.sleep(5)\n",
    "    driver.switch_to.default_content()\n",
    "    driver.quit()\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_permit_urls = get_list_of_urls(url, PATH)\n",
    "# save the urls to a pickle file that can be opened at any time without re-running the code. \n",
    "with open('permit_urls', 'wb') as f:\n",
    "    pickle.dump(all_permit_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved pickle with all the URLs.\n",
    "all_permit_urls_saved = pd.read_pickle('permit_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_applicant(list_of_description):\n",
    "    \"\"\"\n",
    "    Utilizes regex to obtain the applicants name from the list of description\n",
    "\n",
    "    Parameters:\n",
    "        list_of_description (list of str): the description of the webpage derived from the get_description \n",
    "\n",
    "    Returns:\n",
    "        A list of the applicant names \n",
    "    \"\"\"\n",
    "    \n",
    "    regex = r'^[\\s\\S]*?(?=\\s+has applied)'\n",
    "    list_of_applicant = []\n",
    "    for applicant in list_of_description:\n",
    "        try:\n",
    "            applicant_name = applicant[:re.search(regex, applicant).span()[1]]\n",
    "        except:\n",
    "            applicant_name = 'Unknown'\n",
    "        list_of_applicant.append(applicant_name)\n",
    "    return list_of_applicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_status(driver):\n",
    "    \"\"\"\n",
    "    Scrape webpage to obtain the Director of Planning decision\n",
    "\n",
    "    Parameters:\n",
    "        driver: the driver that opened the webpage\n",
    "\n",
    "    Returns:\n",
    "        The application status\n",
    "    \"\"\"  \n",
    "\n",
    "    try:\n",
    "        text_info = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.TAG_NAME,'strong'))).text.split()\n",
    "        if 'Director' in text_info:\n",
    "            if 'approved' in text_info:\n",
    "                return 'Approved'\n",
    "            elif 'cancelled' in text_info:\n",
    "                return 'Cancelled'\n",
    "            elif 'withdrawn' in text_info:\n",
    "                return 'Withdrawn'\n",
    "            else:\n",
    "                return 'Rejected'\n",
    "        else:\n",
    "            return 'In progress'\n",
    "    except:\n",
    "        return 'In progress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permit_status(driver, regex):\n",
    "    \"\"\"\n",
    "    Scrapes webpage to obtain the development permit id\n",
    "\n",
    "    Parameters:\n",
    "        driver: the driver that opened the webpage\n",
    "        regex: the regular expression required to isolate the permit id\n",
    "\n",
    "    Returns:\n",
    "        The development permit id \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        head_text = driver.find_element(By.TAG_NAME, 'h1').text\n",
    "        permit_id = re.search(regex, head_text)[1] \n",
    "    except:\n",
    "        permit_id = 'Unknown'\n",
    "    return permit_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_description(driver):\n",
    "    \"\"\"\n",
    "    Scrapes each webpage to obtain the description of each development permit application\n",
    "\n",
    "    Parameters:\n",
    "        driver: the driver that opened the webpage\n",
    "\n",
    "    Returns:\n",
    "        The description of the development permit application \n",
    "    \"\"\"\n",
    "\n",
    "    text = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.CLASS_NAME, 'truncated-description'))).text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info(list_of_urls, path):\n",
    "    \"\"\"\n",
    "    Scrapes each webpage in the list of urls\n",
    "\n",
    "    Parameters:\n",
    "        list_of_urls (list of str): all the urls to be scraped\n",
    "        path (str): the location of the user's chromedriver\n",
    "\n",
    "    Returns:\n",
    "        A tuple where the 0th element is the list status, the 1st element is the description list, and the 2nd element is the permit list\n",
    "    \"\"\"\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    list_status = []\n",
    "    description_list = []\n",
    "    permit_list = []\n",
    "    error_list = []\n",
    "    regex_permit = r'\\((DP.*?)\\)'\n",
    "    for url in list_of_urls:\n",
    "        driver = webdriver.Chrome(service = Service(path), options = chrome_options)\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            list_status.append(application_status(driver))\n",
    "            permit_list.append(permit_status(driver, regex_permit))\n",
    "            description_list.append(scrape_description(driver))\n",
    "            driver.quit()\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            error_list.append(url)\n",
    "            driver.quit()\n",
    "            time.sleep(3)\n",
    "    return list_status, description_list, permit_list, error_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information = scrape_info(all_permit_urls_saved, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the scraped information\n",
    "with open('scraped_data', 'wb') as f:\n",
    "    pickle.dump(all_information, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Applicant name from the description\n",
    "applicant = get_list_of_applicant(all_information[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe to hold all the necessary information\n",
    "df = pd.DataFrame({\"Applicant_Name\": applicant})\n",
    "df['permit_id'] = all_information[2]\n",
    "df['applicant_status'] = all_information[0]\n",
    "df['description'] = all_information[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the database\n",
    "with open('database', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab first item of current url list, scrape first page of website, check each url of the first page of website until it matches the first item, stop loop, collect urls in a list and extend this list with the old one. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
