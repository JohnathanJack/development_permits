{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.shapeyourcity.ca/development'\n",
    "PATH = \"C:\\Program Files (x86)\\chromedriver-win64\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_urls(webpage,path):\n",
    "    \"\"\"\n",
    "    Scrapes all the URLs of the development permits within the webpage\n",
    "\n",
    "    Parameters:\n",
    "        webpage (str): the url of the webpage\n",
    "        path (str): the location of the user's chromedriver\n",
    "\n",
    "    Returns:\n",
    "        A list of strings which contains all of the development permits url \n",
    "    \"\"\"\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service = Service(path), options=chrome_options)\n",
    "    driver.get(webpage)\n",
    "    # The element is located within an iframe, required to locate the iframe and switch frames\n",
    "    iframe = driver.find_element(By.TAG_NAME, 'iframe')\n",
    "    url_list = []\n",
    "    driver.switch_to.frame(iframe)\n",
    "    page_num = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \".chakra-button.ehq-paginationButton.css-i1louw\")))\n",
    "    last_page_num = int([item.text for item in page_num][-2])\n",
    "    # Scrape all of the urls on each page\n",
    "    for num in range(last_page_num+1):\n",
    "        # Ensures that all the CSS elements are loaded before scraping\n",
    "        urls = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, '.chakra-link.ehq-projectCoverImg.css-1eh7kaa')))\n",
    "        for url in urls:\n",
    "            url_list.append(url.get_attribute('href'))\n",
    "        # After scraping all of the elements, click to the next page if not on the final page\n",
    "        if num < last_page_num+1:\n",
    "            click = driver.find_element(By.CSS_SELECTOR, \".chakra-button.ehq-paginationButton.ehq-paginationNextButton.css-i1louw\")\n",
    "            click.click()\n",
    "        else:\n",
    "            break\n",
    "        time.sleep(5)\n",
    "    driver.switch_to.default_content()\n",
    "    driver.quit()\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_permit_urls = get_list_of_urls(url, PATH)\n",
    "# save the urls to a pickle file that can be opened at any time without re-running the code. \n",
    "with open('permit_urls', 'wb') as f:\n",
    "    pickle.dump(all_permit_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved pickle with all the URLs.\n",
    "all_permit_urls_saved = pd.read_pickle('permit_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_applicant(list_of_description):\n",
    "    \"\"\"\n",
    "    Utilizes regex to obtain the applicants name from the list of description\n",
    "\n",
    "    Parameters:\n",
    "        list_of_description (list of str): the description of the webpage derived from the get_description \n",
    "\n",
    "    Returns:\n",
    "        A list of the applicant names \n",
    "    \"\"\"\n",
    "    \n",
    "    regex = r'^[\\s\\S]*?(?=\\s+has applied)'\n",
    "    list_of_applicant = []\n",
    "    for applicant in list_of_description:\n",
    "        try:\n",
    "            applicant_name = applicant[:re.search(regex, applicant).span()[1]]\n",
    "        except:\n",
    "            applicant_name = 'Unknown'\n",
    "        list_of_applicant.append(applicant_name)\n",
    "    return list_of_applicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_status(driver):\n",
    "    \"\"\"\n",
    "    Scrape webpage to obtain the Director of Planning decision\n",
    "\n",
    "    Parameters:\n",
    "        driver: the driver that opened the webpage\n",
    "\n",
    "    Returns:\n",
    "        The application status\n",
    "    \"\"\"  \n",
    "\n",
    "    try:\n",
    "        text_info = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.TAG_NAME,'strong'))).text.split()\n",
    "        if 'Director' in text_info:\n",
    "            if 'approved' in text_info:\n",
    "                return 'Approved'\n",
    "            elif 'cancelled' in text_info:\n",
    "                return 'Cancelled'\n",
    "            elif 'withdrawn' in text_info:\n",
    "                return 'Withdrawn'\n",
    "            else:\n",
    "                return 'Rejected'\n",
    "        else:\n",
    "            return 'In progress'\n",
    "    except:\n",
    "        return 'In progress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permit_status(driver, regex):\n",
    "    \"\"\"\n",
    "    Scrapes webpage to obtain the development permit id\n",
    "\n",
    "    Parameters:\n",
    "        driver: the driver that opened the webpage\n",
    "        regex: the regular expression required to isolate the permit id\n",
    "\n",
    "    Returns:\n",
    "        The development permit id \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        head_text = driver.find_element(By.TAG_NAME, 'h1').text\n",
    "        permit_id = re.search(regex, head_text)[1] \n",
    "    except:\n",
    "        permit_id = 'Unknown'\n",
    "    return permit_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_description(driver):\n",
    "    \"\"\"\n",
    "    Scrapes each webpage to obtain the description of each development permit application\n",
    "\n",
    "    Parameters:\n",
    "        driver: the driver that opened the webpage\n",
    "\n",
    "    Returns:\n",
    "        The description of the development permit application \n",
    "    \"\"\"\n",
    "\n",
    "    text = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.CLASS_NAME, 'truncated-description'))).text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info(list_of_urls, path):\n",
    "    \"\"\"\n",
    "    Scrapes each webpage in the list of urls\n",
    "\n",
    "    Parameters:\n",
    "        list_of_urls (list of str): all the urls to be scraped\n",
    "        path (str): the location of the user's chromedriver\n",
    "\n",
    "    Returns:\n",
    "        A tuple where the 0th element is the list status, the 1st element is the description list, and the 2nd element is the permit list\n",
    "    \"\"\"\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    list_status = []\n",
    "    description_list = []\n",
    "    permit_list = []\n",
    "    error_list = []\n",
    "    regex_permit = r'\\((DP.*?)\\)'\n",
    "    for url in list_of_urls:\n",
    "        driver = webdriver.Chrome(service = Service(path), options = chrome_options)\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            list_status.append(application_status(driver))\n",
    "            permit_list.append(permit_status(driver, regex_permit))\n",
    "            description_list.append(scrape_description(driver))\n",
    "            driver.quit()\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            error_list.append(url)\n",
    "            driver.quit()\n",
    "            time.sleep(3)\n",
    "    return list_status, description_list, permit_list, error_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information = scrape_info(all_permit_urls_saved, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the scraped information\n",
    "with open('scraped_data', 'wb') as f:\n",
    "    pickle.dump(all_information, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_information = pd.read_pickle('scraped_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(scraped_info):\n",
    "    \"\"\"\n",
    "    Creates a database out of the scraped information \n",
    "\n",
    "    Parameters:\n",
    "        scraped_info (tuple): Information scraped from the function scrape_info\n",
    "\n",
    "    Returns:\n",
    "        A dataframe with the following columns: Applicant_Name, permit_id, applicant_status and description. \n",
    "    \"\"\"\n",
    "\n",
    "    applicant_names = get_list_of_applicant(scraped_info[1])\n",
    "    df = pd.DataFrame({\"Applicant_Name\": applicant_names})\n",
    "    df['permit_id'] = scraped_info[2]\n",
    "    df['applicant_status'] = scraped_info[0]\n",
    "    df['description'] = scraped_info[1]\n",
    "    return df\n",
    "df = create_df(scraped_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the database\n",
    "with open('database', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_url = all_permit_urls_saved[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recent_urls(webpage, path, recent_url):\n",
    "    \"\"\"\n",
    "    Scrapes all the recent URLs of the development permits within the webpage\n",
    "\n",
    "    Parameters:\n",
    "        webpage (str): the url of the webpage\n",
    "        path (str): the location of the user's chromedriver\n",
    "        recent_url (str): the most recent url from the saved list\n",
    "\n",
    "    Returns:\n",
    "        A list of strings which contains all of the development permits url \n",
    "    \"\"\"\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service = Service(path), options=chrome_options)\n",
    "    driver.get(webpage)\n",
    "    # The element is located within an iframe, required to locate the iframe and switch frames\n",
    "    iframe = driver.find_element(By.TAG_NAME, 'iframe')\n",
    "    url_list = []\n",
    "    loop_check = True\n",
    "    driver.switch_to.frame(iframe)\n",
    "    while loop_check:\n",
    "        urls = WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, '.chakra-link.ehq-projectCoverImg.css-1eh7kaa')))\n",
    "        for url in urls:\n",
    "            if url.get_attribute('href') == recent_url:\n",
    "                loop_check = False\n",
    "                break\n",
    "            url_list.append(url.get_attribute('href'))\n",
    "        click = driver.find_element(By.CSS_SELECTOR, \".chakra-button.ehq-paginationButton.ehq-paginationNextButton.css-i1louw\")\n",
    "        click.click()\n",
    "        time.sleep(5)\n",
    "    driver.switch_to.default_content()\n",
    "    driver.quit()\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_database(new_urls, old_df, path):\n",
    "    \"\"\"\n",
    "    Creates a new dataframe and combine it with the old one\n",
    "\n",
    "    Parameters:\n",
    "        new_urls (list of str): the urls of the webpage\n",
    "        old_df (DataFrame): the most recent dataframe\n",
    "        path (str): the location of the user's chromedriver\n",
    "\n",
    "    Returns:\n",
    "        A new dataframe that combined the old with the new additions. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if len(new_urls) == 0:\n",
    "        return old_df\n",
    "    else:\n",
    "        scraped_info = scrape_info(new_urls, path)\n",
    "        new_df = create_df(scraped_info)\n",
    "        update_df = pd.concat([new_df, old_df])\n",
    "        return update_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.shapeyourcity.ca/2560-2580-trafalgar-st-2']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.shapeyourcity.ca/development'\n",
    "new_urls = get_recent_urls(url, PATH, most_recent_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = combine_database(new_urls, df, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(new_urls, old_urls, new_df):\n",
    "    \"\"\"\n",
    "    Creates a new dataframe and combine it with the old one\n",
    "\n",
    "    Parameters:\n",
    "        new_urls (list of str): the urls of the webpage\n",
    "        old_df (DataFrame): the most recent dataframe\n",
    "        path (str): the location of the user's chromedriver\n",
    "\n",
    "    Returns:\n",
    "        Updates the pickle saved files for permit_urls and database  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if len(new_urls) == 0:\n",
    "        print('Database not updated')\n",
    "    else:\n",
    "        new_urls.extend(old_urls)\n",
    "        with open('permit_urls', 'wb') as f:\n",
    "            pickle.dump(new_urls, f)\n",
    "        with open('database', 'wb') as f:\n",
    "            pickle.dump(new_df, f)\n",
    "        print('Database updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database updated\n"
     ]
    }
   ],
   "source": [
    "update_database(new_urls, all_permit_urls_saved, a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
